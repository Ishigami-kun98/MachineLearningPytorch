{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scikit PIPELINES"
      ],
      "metadata": {
        "id": "v5eKENM0jdn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It allows us\n",
        "to fit a model including an arbitrary number of transformation steps and apply it to make predictions\n",
        "about new data."
      ],
      "metadata": {
        "id": "gduL8B66jqF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#DataBase\n",
        "df = pd.read_csv(\n",
        "'https://archive.ics.uci.edu/ml/'\n",
        "'machine-learning-databases'\n",
        "'/breast-cancer-wisconsin/wdbc.data',\n",
        "header=None\n",
        ")"
      ],
      "metadata": {
        "id": "pOIPY2XEj0Di"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "X = df.loc[:, 2:].values\n",
        "y = df.loc[:,1].values\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "r0exKBhEkDCo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify =y, random_state=1)"
      ],
      "metadata": {
        "id": "QJOmfjCglOK2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's suppose now that we want to go from 32 dimensional space to 2 by PCA. <br>\n",
        "Instead of model fitting and data transformation, we can chain the standardScaler Pca and LR in a pipeline."
      ],
      "metadata": {
        "id": "bBemvk-fliiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "pipe_lr = make_pipeline(StandardScaler(),\n",
        "                        PCA(n_components=2),\n",
        "                        LogisticRegression())\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "y_pred = pipe_lr.predict(X_test)\n",
        "test_acc = pipe_lr.score(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJx7MUpVlsFO",
        "outputId": "20b51bd3-12eb-4f40-bef2-e7b5fdad5487"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The make_pipeline takes arbitrary number of scikit-learn transformers and have a estimator that implements the fit and predict methods. <br>\n",
        "In this case we have 2 transformers (SS, PCA) and a estimator (LR). <br>\n",
        "The order of pipe_lr.fit is -> StandardScaler() fit transform -> PCA fit transform -> LogisticRegression fit predict. <br>\n",
        "N.B. if we want to use predict method, the last pipeline element has to be an estimator."
      ],
      "metadata": {
        "id": "f71J-4w9mN3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Holdout"
      ],
      "metadata": {
        "id": "atzfWM3AnNXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split our initial dataset into separate training and test datasets, former for model training, latter to estimate its generalization performance. <br>\n",
        "Model selection -> we want select optimal values of tuning parameters (hyperparameters). Many ppl use test dataset for model selection, this is not a good machine learning practice. <br>\n",
        "Holdout method -> separate the data into 3 parts, training, validation and test dataset. <br>\n",
        "Training dataset is used to fit the different models, and the performance on the validation dataset is then used for model selection.\n",
        "<br> A disadvantages is that the performance is sensitive to how we partition the training dataset into training and validation subsets.\n"
      ],
      "metadata": {
        "id": "bLfzsOlVd2ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1> K-Fold Cross-Validation <br>\n",
        "<H6> Randomly split training dataset into k folds without replacement.\n",
        "k-1 folds are used for model training, and one fold is test fold and is used for performance evaluation. <br>\n",
        "This procedure is repeated k times so that we obtain k models and performance estimates.\n",
        "<br> We use this for model tuning. Finding the optimal hyperparameter values thatyield a satisfying generalization performance.\n",
        "<br> Once we got the best hyperparameter we can retrain using the entire training dataset to obtain a final performance estimate using the independent test dataset.\n",
        "<br> All test folds are disjoint, there is no overlap between test folds.\n",
        "<br> In summary k-fold cross-validation makes better use of the dataset then the holdout method with a validation set, since k-fold cross-validation all data points are being used for evaluation.\n"
      ],
      "metadata": {
        "id": "KNoN7EiXfDy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dont forget to run above code for pipeline\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)    #Kfolds split\n",
        "scores = []\n",
        "for k, (train, test) in enumerate(kfold):       #Format is (k, (train, test))\n",
        "  pipe_lr.fit(X_train[train], y_train[train])\n",
        "  score = pipe_lr.score(X_train[test], y_train[test])\n",
        "  scores.append(score)\n",
        "  print(f'Fold: {k+1:02d}, 'f'Class distr.: {np.bincount(y_train[train])}, 'f'Acc.: {score:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0tU6FNYgdHB",
        "outputId": "b1f3c491-91b3-4feb-ec9c-af10a39df31e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold: 01, Class distr.: [225 133], Acc.: 0.925\n",
            "Fold: 02, Class distr.: [225 133], Acc.: 0.950\n",
            "Fold: 03, Class distr.: [225 133], Acc.: 0.950\n",
            "Fold: 04, Class distr.: [225 133], Acc.: 0.950\n",
            "Fold: 05, Class distr.: [225 133], Acc.: 0.875\n",
            "Fold: 06, Class distr.: [225 133], Acc.: 1.000\n",
            "Fold: 07, Class distr.: [225 133], Acc.: 0.925\n",
            "Fold: 08, Class distr.: [225 133], Acc.: 0.925\n",
            "Fold: 09, Class distr.: [225 134], Acc.: 0.974\n",
            "Fold: 10, Class distr.: [225 134], Acc.: 0.974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_acc = np.mean(scores)\n",
        "std_acc = np.std(scores)\n",
        "print(f'\\nCV accuracy: {mean_acc:.3f} +/- {std_acc:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42Cr8cuFhKwQ",
        "outputId": "c10c4103-56ef-4af6-e36c-c452b5eeaae7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CV accuracy: 0.945 +/- 0.033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SCiKit Scoler for k-fold cross-validation"
      ],
      "metadata": {
        "id": "xraGULO7jFJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=1)\n",
        "print(f'CV accuracy scores: {scores}')\n",
        "print(f'CV accuracy: {np.mean(scores):.3f} 'f'+/- {np.std(scores):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcCmm8MJjJMX",
        "outputId": "f4a83f24-2c65-4369-ab37-3f670162982d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV accuracy scores: [0.925      0.95       0.95       0.95       0.875      1.\n",
            " 0.925      0.925      0.97435897 0.97435897]\n",
            "CV accuracy: 0.945 +/- 0.033\n"
          ]
        }
      ]
    }
  ]
}