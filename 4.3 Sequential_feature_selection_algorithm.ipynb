{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential feature selection algorithms\n"
      ],
      "metadata": {
        "id": "THX8hvxwGTaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An alternative way to reduce complexity of the model to avoid overfitting is dimensionality reduction via feature selection, very useul for unregularized models. <br>\n",
        "There is 2 way, feature selection and extraction. <br>\n",
        "Feature selection -> select a subset of the original features. <br>\n",
        "Feature extraction -> we derive information from the feature set to construct a new feature space. <br>\n"
      ],
      "metadata": {
        "id": "epmZEWSUGbTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1> Greedy search algorithms <br>\n",
        "<h6> Sequential feature selection is a family of greedy search algorithms. <br>\n",
        "Having a d-dimension feature space to k-dimension where k < d. <br>\n",
        "Select the subset of features that are most relevant to the problem.\n",
        "<br> A classical algorithm is sequential backward selection(SBS). This reduce the dimension of the initial feature subspace with a minimum decay in the performance. N.B. greedy search only for local optimum. <br>\n",
        "We need define criterion function J to minimize.\n",
        "at each stage we eliminate the feature that causes the least performance loss after removal. four simple steps: <br>\n",
        "\n",
        "1.   Initialize algorithm with k = d.\n",
        "1.   Determine feature that maximize the criterion\n",
        "2.   Remove the feature that maximize the criterion.\n",
        "2.   If k = nDesiredFeatures end, otherwise go to 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ics6oLRPG7mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import clone\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "class SBS:\n",
        "  def __init__(self, estimator, k_features,\n",
        "                scoring=accuracy_score,\n",
        "                test_size=0.25, random_state=1):\n",
        "    self.scoring = scoring\n",
        "    self.estimator = clone(estimator)\n",
        "    self.k_features = k_features\n",
        "    self.test_size = test_size\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "      train_test_split(X, y, test_size=self.test_size,\n",
        "      random_state=self.random_state)\n",
        "    dim = X_train.shape[1]\n",
        "    self.indices_ = tuple(range(dim))\n",
        "    self.subsets_ = [self.indices_]\n",
        "    score = self._calc_score(X_train, y_train,\n",
        "                              X_test, y_test, self.indices_)\n",
        "    self.scores_ = [score]\n",
        "    while dim > self.k_features:\n",
        "      scores = []\n",
        "      subsets = []\n",
        "      for p in combinations(self.indices_, r=dim - 1):\n",
        "        score = self._calc_score(X_train, y_train,\n",
        "                                 X_test, y_test, p)\n",
        "        scores.append(score)\n",
        "        subsets.append(p)\n",
        "      best = np.argmax(scores)\n",
        "      self.indices_ = subsets[best]\n",
        "      self.subsets_.append(self.indices_)\n",
        "      dim -= 1\n",
        "      self.scores_.append(scores[best])\n",
        "    self.k_score_ = self.scores_[-1]\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    return X[:, self.indices_]\n",
        "  def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
        "    self.estimator.fit(X_train[:, indices], y_train)\n",
        "    y_pred = self.estimator.predict(X_test[:, indices])\n",
        "    score = self.scoring(y_test, y_pred)\n",
        "    return score\n"
      ],
      "metadata": {
        "id": "W5E7Uzx8IUp9"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}